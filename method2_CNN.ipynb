{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3742ddce",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc03001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d95af",
   "metadata": {},
   "source": [
    "Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035f36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_TO_PROCESS = ['dataset1', 'dataset2']\n",
    "PREPROCESSED_DIR = 'preprocessed_data'\n",
    "RESULTS_DIR = 'results'\n",
    "MODELS_DIR = 'models'\n",
    "CNN_MODELS_DIR = os.path.join(MODELS_DIR, 'all_cnn_models') # save all cnn models to a sub directory\n",
    "CM_DIR = os.path.join(RESULTS_DIR, 'confusion_matrices_cnn')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(CNN_MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(CM_DIR, exist_ok=True)\n",
    "\n",
    "final_summary_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aae950",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4571027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_func(y_true, y_pred, class_names, title, output_path):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label'); plt.ylabel('Actual Label')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    print(f'saved confusion matrix to {output_path}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in DATASETS_TO_PROCESS:\n",
    "    print('\\n' + '#' * 80)\n",
    "    print(f'\\nDATASET - {dataset_name.upper()}')\n",
    "    print('\\n' + '#' * 80)\n",
    "\n",
    "    # load preprocessed data\n",
    "    \n",
    "    filepath = os.path.join(PREPROCESSED_DIR, f'{dataset_name}_processed.npz')\n",
    "    with np.load(filepath, allow_pickle=True) as data:\n",
    "        images, labels, class_map = data['images'], data['labels'], data['class_map'].item()\n",
    "    \n",
    "    class_names = list(class_map.keys())\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    images_normalized = images / 255.0\n",
    "    labels_categorical = to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images_normalized, labels_categorical, test_size=0.20, random_state=42, stratify=labels_categorical\n",
    "    )\n",
    "    \n",
    "    # data augmentation layer\n",
    "\n",
    "    data_augmentation = Sequential([\n",
    "        RandomFlip(\"horizontal\"), RandomRotation(0.1), RandomZoom(0.1),\n",
    "    ], name='data_augmentation')\n",
    "\n",
    "    # CNN architecture\n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'\\nbuilding CNN models and optimizing for {dataset_name}')\n",
    "    print('\\n' + '=' * 60)\n",
    "    \n",
    "    conv_blocks_options = [3, 4, 5]\n",
    "    dropout_rate_options = [0.3, 0.5]\n",
    "    learning_rate_options = [0.001, 0.0005]\n",
    "    \n",
    "    # hyperparameter search\n",
    "\n",
    "    for conv_blocks in conv_blocks_options:\n",
    "        for dropout_rate in dropout_rate_options:\n",
    "            for learning_rate in learning_rate_options:\n",
    "                \n",
    "                param_str = f'blocks_{conv_blocks}_dropout_{dropout_rate}_LR_{learning_rate}'\n",
    "                print(f'\\n--- trying {param_str} ---')\n",
    "\n",
    "                model = Sequential(name=param_str)\n",
    "                model.add(tf.keras.Input(shape=X_train.shape[1:]))\n",
    "                model.add(data_augmentation)\n",
    "\n",
    "                for i in range(conv_blocks):\n",
    "                    filters = 32 * (2**i)\n",
    "                    model.add(Conv2D(filters, (3, 3), activation='relu', padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(MaxPooling2D((2, 2)))\n",
    "                \n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "                model.add(Dropout(dropout_rate))\n",
    "                model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "                model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                \n",
    "                early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "\n",
    "                model.fit(\n",
    "                    X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_split=0.2, callbacks=[early_stopping, reduce_lr], verbose=0\n",
    "                )\n",
    "                \n",
    "                _, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                print(f'\\naccuracy for {param_str}: {test_accuracy:.4f}')\n",
    "\n",
    "                # save each combination\n",
    "\n",
    "                model_path = os.path.join(CNN_MODELS_DIR, f'{dataset_name}_{param_str}.h5')\n",
    "                model.save(model_path)\n",
    "                \n",
    "                y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "                y_true = np.argmax(y_test, axis=1)\n",
    "                \n",
    "                # print confusion matrix for the combination\n",
    "            \n",
    "                cm_title = f'CM for {dataset_name}\\n({param_str})'\n",
    "                cm_output_path = os.path.join(CM_DIR, f'CM_{dataset_name}_{param_str}.png')\n",
    "                plot_confusion_matrix_func(y_true, y_pred, class_names, cm_title, cm_output_path)\n",
    "                \n",
    "                report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "                final_summary_list.append({\n",
    "                    'Dataset': dataset_name, 'Model': 'CNN', 'Conv Blocks': conv_blocks,\n",
    "                    'Dropout Rate': dropout_rate, 'Learning Rate': learning_rate,\n",
    "                    'Accuracy': test_accuracy, 'F1-Score (Macro)': report_dict['macro avg']['f1-score'],\n",
    "                    'Precision (Macro)': report_dict['macro avg']['precision'], 'Recall (Macro)': report_dict['macro avg']['recall']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad5b89",
   "metadata": {},
   "source": [
    "Final CNN comparison table for the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '#' * 80)\n",
    "print('\\n SUMMARY TABLE FOR ALL CNN MODELS')\n",
    "print('\\n' + '#' * 80)\n",
    "\n",
    "df_comparison = pd.DataFrame(final_summary_list)\n",
    "df_comparison = df_comparison.sort_values(by=['Dataset', 'Accuracy'], ascending=[True, False])\n",
    "    \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1200)\n",
    "    \n",
    "print(df_comparison.to_string(index=False))\n",
    "    \n",
    "comparison_table_path = os.path.join(RESULTS_DIR, 'method2_cnn_full_summary.csv')\n",
    "df_comparison.to_csv(comparison_table_path, index=False)\n",
    "print(f'\\nSaved CNN summary table to {comparison_table_path}')\n",
    "\n",
    "print('\\n\\nDONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
