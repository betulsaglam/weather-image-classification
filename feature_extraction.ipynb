{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from itertools import product\n",
    "from skimage.feature import local_binary_pattern, hog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55757c64",
   "metadata": {},
   "source": [
    "Basic histograms for each channel, so: H: 180 bins, S: 256 bins, V: 256 bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46343e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hsv_histogram(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    h_hist = cv2.calcHist([hsv], [0], None, [180], [0, 180])\n",
    "    s_hist = cv2.calcHist([hsv], [1], None, [256], [0, 256])\n",
    "    v_hist = cv2.calcHist([hsv], [2], None, [256], [0, 256])\n",
    "    cv2.normalize(h_hist, h_hist)\n",
    "    cv2.normalize(s_hist, s_hist)\n",
    "    cv2.normalize(v_hist, v_hist)\n",
    "    return np.concatenate([h_hist, s_hist, v_hist]).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec2173",
   "metadata": {},
   "source": [
    "Laplacian operator is sensitive to noise so blur the image slightly - I am using a 3x3 kernel to not blur too much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_laplace_variance(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    return np.array([cv2.Laplacian(gray_blurred, cv2.CV_32F).var()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088d6c7",
   "metadata": {},
   "source": [
    "HOG to extract edge/shape information from images.\n",
    "\n",
    "pixels_per_cell will be tuned by trying combinations for (8, 8), (16, 16) since it is the most effective parameter.\n",
    "\n",
    "for other parameters, use the default values to not make the problem too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4595509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image, pixels_per_cell):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return hog(gray, orientations=9, pixels_per_cell=pixels_per_cell, cells_per_block=(3, 3), block_norm='L2-Hys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee6522",
   "metadata": {},
   "source": [
    "LBP to extract texture from images. ['uniform'](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.local_binary_pattern) used to make result rotation invariant and to have a lower dimensional feature vector.\n",
    "\n",
    "lbp_points and lbp_radius will be tuned by trying combinations for lbp_points = 8, 16 and lbp_radius = 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c097dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbp_histogram(image, lbp_points, lbp_radius):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=lbp_points, R=lbp_radius, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=lbp_points + 2, range=(0, lbp_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= hist.sum()\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "908ebe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features_for_image(image, hog_pixels, lbp_points, lbp_radius):\n",
    "    hsv_hist = extract_hsv_histogram(image)\n",
    "    laplace_var = extract_laplace_variance(image)\n",
    "    lbp_hist = extract_lbp_histogram(image, lbp_points, lbp_radius)\n",
    "    hog_feats = extract_hog_features(image, hog_pixels)\n",
    "    all_features = np.hstack([hsv_hist, laplace_var, lbp_hist, hog_feats])\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ee6ac",
   "metadata": {},
   "source": [
    "for each hyperparameter combination, extract features and save in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e581839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---  dataset1 ---\n",
      "\n",
      "processing dataset1_hog8x8_lbp8p1r\n",
      "\n",
      "completed extracting  for 6862 images in 136.74 seconds\n",
      "\n",
      "processing dataset1_hog8x8_lbp8p2r\n",
      "\n",
      "completed extracting  for 6862 images in 127.53 seconds\n",
      "\n",
      "processing dataset1_hog8x8_lbp8p3r\n",
      "\n",
      "completed extracting  for 6862 images in 128.25 seconds\n",
      "\n",
      "processing dataset1_hog8x8_lbp16p1r\n",
      "\n",
      "completed extracting  for 6862 images in 143.33 seconds\n",
      "\n",
      "processing dataset1_hog8x8_lbp16p2r\n",
      "\n",
      "completed extracting  for 6862 images in 144.05 seconds\n",
      "\n",
      "processing dataset1_hog8x8_lbp16p3r\n",
      "\n",
      "completed extracting  for 6862 images in 143.25 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp8p1r\n",
      "\n",
      "completed extracting  for 6862 images in 57.32 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp8p2r\n",
      "\n",
      "completed extracting  for 6862 images in 57.38 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp8p3r\n",
      "\n",
      "completed extracting  for 6862 images in 57.56 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp16p1r\n",
      "\n",
      "completed extracting  for 6862 images in 73.53 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp16p2r\n",
      "\n",
      "completed extracting  for 6862 images in 73.88 seconds\n",
      "\n",
      "processing dataset1_hog16x16_lbp16p3r\n",
      "\n",
      "completed extracting  for 6862 images in 74.13 seconds\n",
      "\n",
      "---  dataset2 ---\n",
      "\n",
      "processing dataset2_hog8x8_lbp8p1r\n",
      "\n",
      "completed extracting  for 3116 images in 56.72 seconds\n",
      "\n",
      "processing dataset2_hog8x8_lbp8p2r\n",
      "\n",
      "completed extracting  for 3116 images in 56.34 seconds\n",
      "\n",
      "processing dataset2_hog8x8_lbp8p3r\n",
      "\n",
      "completed extracting  for 3116 images in 56.76 seconds\n",
      "\n",
      "processing dataset2_hog8x8_lbp16p1r\n",
      "\n",
      "completed extracting  for 3116 images in 64.01 seconds\n",
      "\n",
      "processing dataset2_hog8x8_lbp16p2r\n",
      "\n",
      "completed extracting  for 3116 images in 63.93 seconds\n",
      "\n",
      "processing dataset2_hog8x8_lbp16p3r\n",
      "\n",
      "completed extracting  for 3116 images in 63.83 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp8p1r\n",
      "\n",
      "completed extracting  for 3116 images in 25.81 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp8p2r\n",
      "\n",
      "completed extracting  for 3116 images in 25.77 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp8p3r\n",
      "\n",
      "completed extracting  for 3116 images in 25.60 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp16p1r\n",
      "\n",
      "completed extracting  for 3116 images in 33.35 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp16p2r\n",
      "\n",
      "completed extracting  for 3116 images in 33.44 seconds\n",
      "\n",
      "processing dataset2_hog16x16_lbp16p3r\n",
      "\n",
      "completed extracting  for 3116 images in 33.38 seconds\n"
     ]
    }
   ],
   "source": [
    "DATASETS_TO_PROCESS = ['dataset1', 'dataset2']\n",
    "FEATURES_DIR = 'extracted_features'\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "hog_pixels_options = [(8, 8), (16, 16)]\n",
    "lbp_points_options = [8, 16]\n",
    "lbp_radius_options = [1, 2, 3]\n",
    "all_combinations = list(product(hog_pixels_options, lbp_points_options, lbp_radius_options))\n",
    "\n",
    "\n",
    "for dataset_name in DATASETS_TO_PROCESS:\n",
    "    print(f'\\n---  {dataset_name} ---')\n",
    "    \n",
    "    # load preprocessed data\n",
    "\n",
    "    filepath = os.path.join('preprocessed_data', f'{dataset_name}_processed.npz')\n",
    "    with np.load(filepath, allow_pickle=True) as data:\n",
    "        images, labels, class_map = data['images'], data['labels'], data['class_map'].item()\n",
    "\n",
    "    for hog_pixels, lbp_points, lbp_radius in all_combinations:\n",
    "        \n",
    "        # create file name to save\n",
    "\n",
    "        hog_str = f'hog{hog_pixels[0]}x{hog_pixels[1]}'\n",
    "        lbp_str = f'lbp{lbp_points}p{lbp_radius}r'\n",
    "        feature_set_name = f'{dataset_name}_{hog_str}_{lbp_str}'\n",
    "        output_path = os.path.join(FEATURES_DIR, f'{feature_set_name}.npz')\n",
    "        \n",
    "        print(f'\\nprocessing {feature_set_name}')\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print('\\nfeatures already extracted for this combination, skipping...')\n",
    "            continue\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        features_list = []\n",
    "        for image in images:\n",
    "            features = extract_all_features_for_image(image, hog_pixels, lbp_points, lbp_radius)\n",
    "            features_list.append(features)\n",
    "        \n",
    "        features_array = np.array(features_list)\n",
    "        \n",
    "        np.savez_compressed(output_path, features=features_array, labels=labels, class_map=class_map)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f'\\ncompleted extracting  for {len(images)} images in {end_time - start_time:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
